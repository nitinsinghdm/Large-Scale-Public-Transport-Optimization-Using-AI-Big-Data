# -*- coding: utf-8 -*-
"""Large-Scale PTO Dissertation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nWOu4ToWndedKnvL5jrwVMATVdgZz4N0
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

# Base path where GTFS files are stored
base_path = '/content/drive/MyDrive/Dissertation/GFTS/'

# Load GTFS CSVs
trips_df = pd.read_csv(base_path + 'trips.csv')
stop_times_df = pd.read_csv(base_path + 'stop_times.csv')
routes_df = pd.read_csv(base_path + 'routes.csv')
calendar_df = pd.read_csv(base_path + 'calendar.csv')
calendar_dates_df = pd.read_csv(base_path + 'calendar_dates.csv')
stops_df = pd.read_csv(base_path + 'stops.csv')
shapes_df = pd.read_csv(base_path + 'shapes.csv')
transfers_df = pd.read_csv(base_path + 'transfers.csv')
levels_df = pd.read_csv(base_path + 'levels.csv')
pathways_df = pd.read_csv(base_path + 'pathways.csv')

"""# **Regression ‚Äì Predict Delay in Minutes**"""

# Force arrival_time to string and handle time properly
stop_times_df['arrival_time'] = stop_times_df['arrival_time'].astype(str)
stop_times_df['departure_time'] = stop_times_df['departure_time'].astype(str)

def time_to_seconds(t):
    try:
        h, m, s = map(int, t.split(":"))
        return h * 3600 + m * 60 + s
    except:
        return None  # For missing or corrupted entries

# Create arrival and departure in seconds
stop_times_df['arrival_secs'] = stop_times_df['arrival_time'].apply(time_to_seconds)
stop_times_df['departure_secs'] = stop_times_df['departure_time'].apply(time_to_seconds)

# Group to get duration and number of stops per trip
trip_summary = stop_times_df.groupby('trip_id').agg({
    'arrival_secs': 'first',
    'departure_secs': 'last',
    'stop_id': 'count'
}).reset_index().rename(columns={
    'stop_id': 'number_of_stops'
})

trip_summary['trip_duration_secs'] = trip_summary['departure_secs'] - trip_summary['arrival_secs']
trip_summary['trip_duration_mins'] = trip_summary['trip_duration_secs'] / 60

# Merge with trips
merged = trips_df.merge(trip_summary[['trip_id', 'trip_duration_mins', 'number_of_stops']], on='trip_id', how='left')

# Merge with routes
merged = merged.merge(routes_df[['route_id', 'route_type']], on='route_id', how='left')

# Merge with calendar
calendar_df['start_date'] = pd.to_datetime(calendar_df['start_date'], format='%Y%m%d')
merged = merged.merge(calendar_df, on='service_id', how='left')

# Add day_of_week and weekend info
merged['day_of_week'] = merged['start_date'].dt.dayofweek
merged['is_weekend'] = merged['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)

import numpy as np

# Simulate random delays (normally distributed around 5 mins, std dev 2)
np.random.seed(42)  # for reproducibility
merged['delay_minutes'] = np.random.normal(loc=5, scale=2, size=len(merged)).clip(min=0)

# Binary classification: delayed if delay > 5 minutes
merged['is_delayed'] = merged['delay_minutes'].apply(lambda x: 1 if x > 5 else 0)

#Prepare the data
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Features and target
features = ['trip_duration_mins', 'number_of_stops', 'route_type', 'day_of_week', 'is_weekend']
target = 'delay_minutes'

X = merged[features]
y = merged[target]

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Train the model
# Initialize and train Random Forest Regressor
rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)
rf_regressor.fit(X_train, y_train)

# Predict
y_pred = rf_regressor.predict(X_test)

#Evaluate the model
import numpy as np

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"Mean Absolute Error (MAE): {mae:.2f} minutes")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f} minutes")
print(f"R¬≤ Score: {r2:.2f}")

#Feature Importance Plot
import matplotlib.pyplot as plt

importances = rf_regressor.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 7))
plt.title("Feature Importance (Regression)")
plt.bar([features[i] for i in indices], importances[indices])
plt.ylabel("Importance")
plt.show()

"""# **Classification ‚Äî Predicting Delayed vs. Not Delayed**"""

# Calculate trip duration and stop count from stop_times.csv
stop_times_df['arrival_time'] = pd.to_timedelta(stop_times_df['arrival_time'])
stop_times_df['departure_time'] = pd.to_timedelta(stop_times_df['departure_time'])

trip_group = stop_times_df.groupby('trip_id')
trip_duration = (trip_group['arrival_time'].max() - trip_group['departure_time'].min()).dt.total_seconds() / 60  # in minutes
number_of_stops = trip_group.size()

# Merge trip info
trip_info_df = trips_df[['trip_id', 'route_id', 'service_id']].drop_duplicates()

# Merge with route type from routes.csv
trip_info_df = trip_info_df.merge(routes_df[['route_id', 'route_type']], on='route_id', how='left')

# Add calendar date for each service
calendar_dates_df['date'] = pd.to_datetime(calendar_dates_df['date'], format='%Y%m%d')
service_dates_df = calendar_dates_df[calendar_dates_df['exception_type'] == 1][['service_id', 'date']]  # only added services
trip_info_df = trip_info_df.merge(service_dates_df, on='service_id', how='left')

# Feature: day_of_week and is_weekend
trip_info_df['day_of_week'] = trip_info_df['date'].dt.dayofweek
trip_info_df['is_weekend'] = trip_info_df['day_of_week'].isin([5, 6]).astype(int)

# Final feature set
features_df = pd.DataFrame({
    'trip_duration_mins': trip_info_df['trip_id'].map(trip_duration),
    'number_of_stops': trip_info_df['trip_id'].map(number_of_stops),
    'route_type': trip_info_df['route_type'],
    'is_weekend': trip_info_df['is_weekend'],
    'day_of_week': trip_info_df['day_of_week']
}).dropna()

# Simulate delays (e.g., based on long trip duration or randomness)
np.random.seed(42)
features_df['delay_minutes'] = np.where(
    features_df['trip_duration_mins'] > features_df['trip_duration_mins'].median(),
    np.random.randint(4, 10, size=len(features_df)),
    np.random.randint(0, 4, size=len(features_df))
)

features_df.reset_index(drop=True, inplace=True)

print("‚úÖ features_df created with shape:", features_df.shape)
features_df.head()

# Define binary target (1 = delayed if delay > 5 mins)
features_df['is_delayed'] = (features_df['delay_minutes'] > 5).astype(int)

from sklearn.model_selection import train_test_split

X_cls = features_df[['trip_duration_mins', 'number_of_stops', 'route_type', 'is_weekend', 'day_of_week']]
y_cls = features_df['is_delayed']

# Train-test split
X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(X_cls, y_cls, test_size=0.2, random_state=42)

from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train_cls, y_train_cls)

# Predictions
y_pred_cls = clf.predict(X_test_cls)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

print("Accuracy:", accuracy_score(y_test_cls, y_pred_cls))
print("Precision:", precision_score(y_test_cls, y_pred_cls))
print("Recall:", recall_score(y_test_cls, y_pred_cls))
print("F1 Score:", f1_score(y_test_cls, y_pred_cls))
print("\nClassification Report:\n", classification_report(y_test_cls, y_pred_cls))

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test_cls, y_pred_cls)

sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['On Time', 'Delayed'], yticklabels=['On Time', 'Delayed'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix ‚Äì Delay Classification')
plt.show()

"""# **Route Optimization**"""

import networkx as nx
from datetime import datetime, timedelta

# Load necessary files
stop_times_df = pd.read_csv(base_path + 'stop_times.csv')
stops_df = pd.read_csv(base_path + 'stops.csv')

# Convert departure_time to minutes since midnight
def time_to_minutes(t):
    try:
        h, m, s = map(int, t.split(':'))
        return h * 60 + m + s / 60
    except:
        return None

stop_times_df['departure_mins'] = stop_times_df['departure_time'].apply(time_to_minutes)
stop_times_df = stop_times_df.dropna(subset=['departure_mins'])

# Sort by trip_id and stop_sequence to link stop pairs
stop_times_df = stop_times_df.sort_values(['trip_id', 'stop_sequence'])

# Build graph
G = nx.DiGraph()

for trip_id, group in stop_times_df.groupby('trip_id'):
    stops = group[['stop_id', 'departure_mins']].values
    for i in range(len(stops) - 1):
        u, t1 = stops[i]
        v, t2 = stops[i+1]
        duration = max(t2 - t1, 0.5)  # avoid 0-weight edges
        if G.has_edge(u, v):
            G[u][v]['weight'].append(duration)
        else:
            G.add_edge(u, v, weight=[duration])

# Convert weight list to average travel time
for u, v in G.edges():
    G[u][v]['weight'] = sum(G[u][v]['weight']) / len(G[u][v]['weight'])

print(f"Graph created with {G.number_of_nodes()} stops and {G.number_of_edges()} connections.")

# Check top 20 most frequent stop names
stops_df['stop_name'].value_counts().head(20)

# Get stop_ids for both source and destination
source_name = "S+U Berlin Hauptbahnhof"
target_name = "S+U Alexanderplatz Bhf (Berlin)"

# Find all matching stop_ids
source_stops = stops_df[stops_df['stop_name'] == source_name]
target_stops = stops_df[stops_df['stop_name'] == target_name]

print("Source Stops:")
print(source_stops[['stop_id', 'stop_name']])
print("\nDestination Stops:")
print(target_stops[['stop_id', 'stop_name']])

source_stop_id = 'de:11000:900003201:1:51'
target_stop_id = 'de:11000:900100003:1:51'

try:
    shortest_path = nx.dijkstra_path(G, source=source_stop_id, target=target_stop_id, weight='weight')
    total_time = nx.dijkstra_path_length(G, source=source_stop_id, target=target_stop_id, weight='weight')

    print("üó∫Ô∏è Fastest Route from Hauptbahnhof to Alexanderplatz:\n")
    for stop in shortest_path:
        stop_name = stops_df.loc[stops_df['stop_id'] == stop, 'stop_name'].values
        if len(stop_name) > 0:
            print(f"{stop} ‚Üí {stop_name[0]}")
        else:
            print(f"{stop} ‚Üí (Name not found)")

    print(f"\n‚è±Ô∏è Estimated Total Travel Time: {total_time:.2f} minutes")

except nx.NetworkXNoPath:
    print("‚ùå No path found between the selected stops.")

import folium

# Extract lat/lon for each stop in the path
route_stops = stops_df[stops_df['stop_id'].isin(shortest_path)][['stop_id', 'stop_name', 'stop_lat', 'stop_lon']]

# Sort route_stops in order of the shortest_path
route_stops['stop_order'] = route_stops['stop_id'].apply(lambda x: shortest_path.index(x))
route_stops = route_stops.sort_values('stop_order')

# Create a folium map centered at the first stop
start_coords = [route_stops.iloc[0]['stop_lat'], route_stops.iloc[0]['stop_lon']]
m = folium.Map(location=start_coords, zoom_start=13)

# Add markers for each stop
for i, row in route_stops.iterrows():
    folium.Marker(
        location=[row['stop_lat'], row['stop_lon']],
        popup=row['stop_name'],
        tooltip=f"{i+1}. {row['stop_name']}"
    ).add_to(m)

# Draw the route as a polyline
folium.PolyLine(
    locations=route_stops[['stop_lat', 'stop_lon']].values.tolist(),
    color='blue',
    weight=5,
    opacity=0.7
).add_to(m)

m

# View the first few rows
route_stops.head(10)

# Save as CSV to your Google Drive folder
route_stops.to_csv('/content/drive/MyDrive/Dissertation/optimized_route_hbf_to_alexanderplatz.csv', index=False)

print("‚úÖ Route exported to: Dissertation/optimized_route_hbf_to_alexanderplatz.csv")